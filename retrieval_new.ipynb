{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from collections import Counter\n",
    "\n",
    "# Parse the XML data\n",
    "def get_text(doc_path):\n",
    "    tree = ET.parse(doc_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Extract metadata\n",
    "    metadata = root.find('./teiHeader')\n",
    "    metadata_dict = {}\n",
    "    for item in metadata.findall('metadata'):\n",
    "        name = item.get('name')\n",
    "        value = item.text\n",
    "        metadata_dict[name] = value\n",
    "\n",
    "    # Extract text content\n",
    "    text = root.find('./text')\n",
    "    body_author = text.find('./body').get('author')\n",
    "    title_author = text.find('./title').get('author')\n",
    "    sentences = text.findall('body/s')\n",
    "    comments = text.findall('comment')\n",
    "    comments_pairs = [([(word.get('type'), word.text) for word in c.findall('s/w')], c.get('c_type')) for c in comments]\n",
    "    sentences_pairs = [[(word.get('type'), word.text) for word in sent.findall('w')] for sent in sentences]\n",
    "    text = []\n",
    "    c = Counter()\n",
    "    for sentense in sentences_pairs:\n",
    "        sentense_parsed = ''.join([word[1] for word in sentense])\n",
    "        text.append(sentense_parsed)\n",
    "    text = '\\n'.join(text)\n",
    "    com = []\n",
    "    for comment in comments_pairs:\n",
    "        c[comment[1]] += 1\n",
    "        t = []\n",
    "        for word in comment[0]:\n",
    "            t.append(word[1])\n",
    "        com.append(''.join(t))\n",
    "    file_name = doc_path.split('/')[-1]\n",
    "    return {'date': file_name[:6], 'text': text, 'pos': c['pos'], 'neu': c['neu'], 'neg': c['neg'], 'comment': '\\n'.join(com)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc76414ea8dd45978f470fc7bec2207f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd7948e11c5429688c8c9572ec7ebe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3815 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73ea22ebb3524447b33e0a3524402a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3780 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed9c926b0caa4a39b37979dd4b858149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af9ed00b0895455ca986b2653af14e4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1178 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "directory = '/nfs/nas-6.1/wclu/cllt/ptt_data/HatePolitics'\n",
    "data = []\n",
    " \n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for filename in tqdm(files):\n",
    "        if filename != '.DS_Store':\n",
    "            doc_path = os.path.join(root, filename)\n",
    "            try:\n",
    "                data.append(get_text(doc_path))\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "import pandas as pd\n",
    "data_df = pd.DataFrame(data)\n",
    "data_df['num_com'] = data_df['pos'] + data_df['neu'] + data_df['neg']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 13:24:05.552928: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-30 13:24:06.472680: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from ckip_transformers.nlp import CkipWordSegmenter\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "ws_driver  = CkipWordSegmenter(model=\"bert-base\", device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def retrieve_bm25(year, month, query):\n",
    "    date = year + month\n",
    "    df = data_df[data_df['date']==date].reset_index(drop=True)\n",
    "    corpus = df['text'].tolist()\n",
    "    corpus_tokenized = ws_driver(corpus, batch_size=64, max_length=509)\n",
    "    bm25 = BM25Okapi(corpus_tokenized)\n",
    "    tokenized_query = ws_driver([query], batch_size=1, max_length=509)[0]\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    top_n = np.argsort(scores)[::-1][:20]\n",
    "    rel_doc = df.iloc[top_n].sort_values(by=['num_com'], ascending=False).head(5)\n",
    "    return rel_doc.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer=BertTokenizer.from_pretrained('IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment', cache_dir='/nfs/nas-6.1/wclu/cache')\n",
    "model=BertForSequenceClassification.from_pretrained('IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment', cache_dir='/nfs/nas-6.1/wclu/cache')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: 'negative',\n",
    "    1: 'positive'\n",
    "}\n",
    "def sentiment_analysis(year, month, query):\n",
    "    rel_doc = retrieve_bm25(year, month, query)\n",
    "    com_text = rel_doc['comment'].tolist()\n",
    "    x = tokenizer(com_text, padding='longest', truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    output = model(x['input_ids'])\n",
    "    sentiment = []\n",
    "    for logit in output.logits:\n",
    "        sentiment.append(id2label[int(logit.argmax())])\n",
    "    rel_doc['senti'] = sentiment\n",
    "    return sentiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def retrieve_tool(year, month, query):\n",
    "    rel_doc = retrieve_bm25(year, month, query)\n",
    "    return '\\n'.join(rel_doc['text'].tolist())[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ckip_ner import ner_person\n",
    "\n",
    "def ner_tool(year, month):\n",
    "    date = year + month\n",
    "    df = data_df[data_df['date']==date].reset_index(drop=True)\n",
    "    corpus = df['text'].tolist()\n",
    "    return ner_person(corpus)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import things that are needed generically\n",
    "from langchain import LLMMathChain, SerpAPIWrapper\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import BaseTool, StructuredTool, Tool, tool\n",
    "\n",
    "openai_api_key='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'\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "tools = [\n",
    "    StructuredTool.from_function(\n",
    "        func=retrieve_tool,\n",
    "        name = \"retrieval\",\n",
    "        description=\"Useful for when you need to retrieve information on specific date. The input of month should be number of two digit. For example, February: 02.\"\n",
    "    ),\n",
    "    StructuredTool.from_function(\n",
    "        func=sentiment_analysis,\n",
    "        name = \"sentiment analysis\",\n",
    "        description=\"Useful for when you need to know the sentiment on specific date. The input of month should be number of two digit. For example, February: 02.\"\n",
    "    ),\n",
    "    StructuredTool.from_function(\n",
    "        func=ner_tool,\n",
    "        name = \"named entity recognition\",\n",
    "        description=\"Useful for when you need to know the most popular people on specific date. The input of month should be number of two digit. For example, February: 02.\"\n",
    "    ),\n",
    "]\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = '''\n",
    "I want to know the most popular people in February, 2023. \n",
    "Also, please tell me some information about the person at that time, and tell me the sentiment toward him. \n",
    "Reply in traditional Chinese.\n",
    "'''\n",
    "prompt2 = '''\n",
    "I want to konw the most popular people in January, 2022.\n",
    "Reply in traditional Chinese.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction:\n",
      "```\n",
      "{\n",
      "  \"action\": \"named entity recognition\",\n",
      "  \"action_input\": {\n",
      "    \"year\": \"2023\",\n",
      "    \"month\": \"02\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1020/1020 [00:01<00:00, 804.59it/s]\n",
      "Inference: 100%|██████████| 23/23 [00:10<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[38;5;200m\u001b[1;3m林智堅\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction:\n",
      "```\n",
      "{\n",
      "  \"action\": \"retrieval\",\n",
      "  \"action_input\": {\n",
      "    \"year\": \"2023\",\n",
      "    \"month\": \"02\",\n",
      "    \"query\": \"林智堅\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1020/1020 [00:04<00:00, 231.19it/s]\n",
      "Inference: 100%|██████████| 23/23 [00:11<00:00,  2.07it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 13934.56it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 22.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3m1.新聞網址︰\n",
      "https://newtalk.tw/news/view/2023-02-13/857325\n",
      "2.新聞來源︰\n",
      "Newtalk\n",
      "3.完整新聞標題\n",
      "不大聲說了嗎？民進黨闢謠專區悄悄下架「林智堅沒有抄襲」頁面\n",
      "4.完整新聞內容︰\n",
      "前新竹市長林智堅論文抄襲案，目前雙雙進入司法程序，台大國發所論文1月中旬台北地\n",
      "院首度召開準備程序庭，傳喚被告林智堅到庭，中華大學論文也被國科會證實，竹科管理\n",
      "局已依法處理，且已進入司法程序。去年還是民進黨主席的總統蔡英文，曾要全體黨公職\n",
      "大聲地跟大家說「林智堅沒有抄襲」，並在民進黨網站闢謠專區中放上為林智堅辯護的頁\n",
      "面，不過，近日被發現已將林智堅論文相關貼文下架。\n",
      "針對林智堅論文抄襲爭議，總統蔡英文去年8月曾要求全體黨公職表態，「只要是完整看\n",
      "過兩本論文，而且完整了解事情來龍去脈的人，都願意選擇相信智堅沒有抄襲」，還呼籲\n",
      "黨公職一起「大聲地跟大家說」，讓社會可以理解整件事情的始末和真相。\n",
      "民進黨也在黨的網站「闢謠專區」頁面中，放上了「林智堅論文為原創」、「林智堅中華\n",
      "大學碩論享有著作權 且口試早於著作權轉移 沒有抄襲、欽權疑慮」等貼文，試圖為林智\n",
      "堅辯護。\n",
      "不過，隨著林智堅論文案先後被台灣大學與中華大學學倫會判定抄襲，案件也在調查局新\n",
      "竹市調查站調查官余正煌正式開告後進入司法程序，資深媒體人黃揚明9日曾在臉書發文\n",
      "表示，民進黨官網的「闢謠專區」仍留有堅稱前新竹市長林智堅論文沒抄襲的2篇文章。\n",
      "黃揚明呼籲，檢討學倫機制之外，也該把這些顛倒是非的內容下架。\n",
      "經過了四天，民進黨原先的「闢謠專區」頁面中，已悄悄將林智堅論文沒有抄襲，且為原\n",
      "創等貼文頁面下架。\n",
      "5.附註、心得、想法︰\n",
      "闢謠闢到全黨火葬場了吼\n",
      "是不是收到風聲台大的結果要出來了\n",
      "看過小智兩本論文的人現在還有呼吸嗎\n",
      "還好我下架前看過這原文了\n",
      "綠共吃屎\n",
      "1.新聞網址︰\n",
      "※ 網址超過一行 請縮網址 ※\n",
      "https://newtalk.tw/news/view/2023-02-15/857533\n",
      "2.新聞來源︰  新頭殼\n",
      "3.完整新聞標題\n",
      "※ 請完整轉載標題 請勿修改與編排 ※\n",
      "翁達瑞：若連林智堅都無法保護 平民百姓受冤屈豈更不能寄望民進黨\n",
      "4.完整新聞內容︰\n",
      "※ 請完整轉載原文 請勿修改內文與編排 ※\n",
      "前新竹市長林智堅論文抄襲風波被民進黨視為是九合一選舉大敗的主因之一，並在調查局\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction:\n",
      "```\n",
      "{\n",
      "  \"action\": \"retrieval\",\n",
      "  \"action_input\": {\n",
      "    \"year\": \"2023\",\n",
      "    \"month\": \"02\",\n",
      "    \"query\": \"翁達瑞\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1020/1020 [00:00<00:00, 1969.14it/s]\n",
      "Inference: 100%|██████████| 23/23 [00:10<00:00,  2.17it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 19784.45it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 141.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3m如題\n",
      "蛋價又來到一個高點\n",
      "然後陳吉仲只會一個雖小臉\n",
      "沒有半點作為\n",
      "然後塔綠班又會說不是缺蛋\n",
      "是缺便宜的蛋而已啦\n",
      "薪水都漲多少了\n",
      "你們吃不起？\n",
      "以後早餐店會不會一顆要20\n",
      "反正翁達瑞說痛苦指數要改名\n",
      "台灣真是太棒了\n",
      "什麼都可以戳螺旋丸\n",
      "我也不懂一種人的心態\n",
      "這裡是政黑\n",
      "發文沒政黑點是3小\n",
      "開文的都知道什麼叫政黑點\n",
      "回文的把這裡當日旅\n",
      "就好像綠共側翼們在網路上罵台北燈會\n",
      "然後被前議員呱吉一句：中央主責。一樣\n",
      "你們跑錯棚了八\n",
      "小心我會學綠粉男神、翁達瑞\n",
      "不要把這裡當日旅，搞清楚再發文\n",
      "再有這種顛倒政黑的發文，我就水桶你\n",
      "https://i.imgur.com/PUt1jXh.jpg\n",
      "不過還好我放假惹\n",
      "你們暫時不會被桶\n",
      "麻煩叔叔阿姨回去把政黑點補上\n",
      "不然就\n",
      "https://i.imgur.com/JkSGOWO.jpg\n",
      "1.新聞網址︰\n",
      "※ 網址超過一行 請縮網址 ※\n",
      "https://newtalk.tw/news/view/2023-02-15/857533\n",
      "2.新聞來源︰  新頭殼\n",
      "3.完整新聞標題\n",
      "※ 請完整轉載標題 請勿修改與編排 ※\n",
      "翁達瑞：若連林智堅都無法保護 平民百姓受冤屈豈更不能寄望民進黨\n",
      "4.完整新聞內容︰\n",
      "※ 請完整轉載原文 請勿修改內文與編排 ※\n",
      "前新竹市長林智堅論文抄襲風波被民進黨視為是九合一選舉大敗的主因之一，並在調查局\n",
      "新竹市調查站調查官余正煌提告後進入司法程序。林智堅14日為學倫爭議紛擾及寫作瑕疵\n",
      "向社會致歉，同時宣布決定向教育部撤回對台大學位的訴願。對此，旅美學者翁達瑞為他\n",
      "叫屈：「若民進黨連林智堅都無法保護，那平民百姓受到冤屈時，豈更不能寄望民進黨的\n",
      "保護」。\n",
      "翁達瑞在臉書Po文說：「教育部駁回中華的學倫調查訴願，林智堅決定不提行政訴訟。林\n",
      "智堅也決定撤回在教育部的台大學倫調查訴願。這表示林智堅選擇吞下他的冤屈。對林智\n",
      "堅的決定，我尊重，但仍感到萬分不捨。林智堅在中華與台大的碩士論文，撰寫的過程『\n",
      "或有瑕疵』，但問題絕對不在抄襲。兩校以抄襲的罪名撤銷林智堅的學位，皆是不可承受\n",
      "的冤屈。」\n",
      "翁達瑞指出：「中華與台大做出抄襲判定後，林智堅面臨龐大的壓力。黨內有人要他認錯\n",
      "道歉，避免波及選情。林智堅最後決定退選，以平民身分找回清白。儘管選舉已結束，林\n",
      "智堅面對的壓力沒有減輕。敗選之後，民進黨內『清理戰場』的聲音擴大。林智堅反變成\n",
      "敗選的戰犯。」\n",
      "翁\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction:\n",
      "```\n",
      "{\n",
      "  \"action\": \"named entity recognition\",\n",
      "  \"action_input\": {\n",
      "    \"year\": \"2023\",\n",
      "    \"month\": \"02\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1020/1020 [00:00<00:00, 1295.87it/s]\n",
      "Inference: 100%|██████████| 23/23 [00:10<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[38;5;200m\u001b[1;3m林智堅\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction:\n",
      "```\n",
      "{\n",
      "  \"action\": \"sentiment analysis\",\n",
      "  \"action_input\": {\n",
      "    \"year\": \"2023\",\n",
      "    \"month\": \"02\",\n",
      "    \"query\": \"林智堅\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1020/1020 [00:00<00:00, 1901.42it/s]\n",
      "Inference: 100%|██████████| 23/23 [00:10<00:00,  2.15it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 21183.35it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 141.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3m['negative', 'positive', 'negative', 'negative', 'negative']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"2023年2月最受歡迎的人物是林智堅。在2月13日，有一篇新聞報導了林智堅的論文抄襲案，並且指出民進黨闢謠專區已經下架了林智堅沒有抄襲的頁面。在2月15日，翁達瑞在臉書上發文表示對林智堅的支持，並且批評民進黨無法保護林智堅。根據情感分析工具，林智堅在2月份的情感傾向為負面。\"\n",
      "}\n",
      "```\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2023年2月最受歡迎的人物是林智堅。在2月13日，有一篇新聞報導了林智堅的論文抄襲案，並且指出民進黨闢謠專區已經下架了林智堅沒有抄襲的頁面。在2月15日，翁達瑞在臉書上發文表示對林智堅的支持，並且批評民進黨無法保護林智堅。根據情感分析工具，林智堅在2月份的情感傾向為負面。'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(prompt1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "embedder = SentenceTransformer('GanymedeNil/text2vec-large-chinese', device=2 ,cache_folder='/nfs/nas-6.1/wclu/cache')\n",
    "\n",
    "corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True, normalize_embeddings=True, device=2, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_semantic(query):\n",
    "    query_embedding = embedder.encode([query], convert_to_tensor=True, normalize_embeddings=True, device=2, show_progress_bar=True)\n",
    "    hits = util.semantic_search(query_embedding, corpus_embeddings, score_function=util.dot_score, top_k=10)\n",
    "    rel_doc = [corpus[hit['corpus_id']] for hit in hits[0]]\n",
    "    return rel_doc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
