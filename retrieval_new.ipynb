{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from collections import Counter\n",
    "\n",
    "# Parse the XML data\n",
    "def get_text(doc_path):\n",
    "    tree = ET.parse(doc_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Extract metadata\n",
    "    metadata = root.find('./teiHeader')\n",
    "    metadata_dict = {}\n",
    "    for item in metadata.findall('metadata'):\n",
    "        name = item.get('name')\n",
    "        value = item.text\n",
    "        metadata_dict[name] = value\n",
    "\n",
    "    # Extract text content\n",
    "    text = root.find('./text')\n",
    "    body_author = text.find('./body').get('author')\n",
    "    title_author = text.find('./title').get('author')\n",
    "    sentences = text.findall('body/s')\n",
    "    comments = text.findall('comment')\n",
    "    comments_pairs = [([(word.get('type'), word.text) for word in c.findall('s/w')], c.get('c_type')) for c in comments]\n",
    "    sentences_pairs = [[(word.get('type'), word.text) for word in sent.findall('w')] for sent in sentences]\n",
    "    text = []\n",
    "    c = Counter()\n",
    "    for sentense in sentences_pairs:\n",
    "        sentense_parsed = ''.join([word[1] for word in sentense])\n",
    "        text.append(sentense_parsed)\n",
    "    text = '\\n'.join(text)\n",
    "    for comment in comments_pairs:\n",
    "        c[comment[1]] += 1\n",
    "    file_name = doc_path.split('/')[-1]\n",
    "    return {'date': file_name[:6], 'text': text, 'pos': c['pos'], 'neu': c['neu'], 'neg': c['neg']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "747033c805fd4343979fba87a7c12869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ce4c1e861747b8b26b45e74ef3b493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3815 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c54f01514ea44b77a56c031e8937d9f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3780 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35928d26812549d5a8101dfddb021e1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a99941433b34ebdb6d87ddfed48efc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1178 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "directory = '/nfs/nas-6.1/wclu/cllt/ptt_data/HatePolitics'\n",
    "data = []\n",
    " \n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for filename in tqdm(files):\n",
    "        if filename != '.DS_Store':\n",
    "            doc_path = os.path.join(root, filename)\n",
    "            try:\n",
    "                data.append(get_text(doc_path))\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "import pandas as pd\n",
    "data_df = pd.DataFrame(data)\n",
    "data_df['num_com'] = data_df['pos'] + data_df['neu'] + data_df['neg']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 13:24:05.552928: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-30 13:24:06.472680: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from ckip_transformers.nlp import CkipWordSegmenter\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "ws_driver  = CkipWordSegmenter(model=\"bert-base\", device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def retrieve_bm25(year, month, query):\n",
    "    date = year + month\n",
    "    df = data_df[data_df['date']==date].reset_index(drop=True)\n",
    "    corpus = df['text'].tolist()\n",
    "    corpus_tokenized = ws_driver(corpus, batch_size=64, max_length=509)\n",
    "    bm25 = BM25Okapi(corpus_tokenized)\n",
    "    tokenized_query = ws_driver([query], batch_size=1, max_length=509)[0]\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    top_n = np.argsort(scores)[::-1][:20]\n",
    "    rel_doc = df.iloc[top_n].sort_values(by=['num_com'], ascending=False).head(5)\n",
    "    return rel_doc.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer=BertTokenizer.from_pretrained('IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment', cache_dir='/nfs/nas-6.1/wclu/cache')\n",
    "model=BertForSequenceClassification.from_pretrained('IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment', cache_dir='/nfs/nas-6.1/wclu/cache')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: 'negative',\n",
    "    1: 'positive'\n",
    "}\n",
    "def sentiment_analysis(year, month, query):\n",
    "    rel_doc = retrieve_bm25(year, month, query)\n",
    "    doc_text = rel_doc['text'].tolist()\n",
    "    x = tokenizer(doc_text, padding='longest', truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    output = model(x['input_ids'])\n",
    "    sentiment = []\n",
    "    for logit in output.logits:\n",
    "        sentiment.append(id2label[int(logit.argmax())])\n",
    "    rel_doc['senti'] = sentiment\n",
    "    return sentiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def retrieve_tool(year, month, query):\n",
    "    rel_doc = retrieve_bm25(year, month, query)\n",
    "    return '\\n'.join(rel_doc['text'].tolist())[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ckip_ner import ner_person\n",
    "\n",
    "def ner_tool(year, month):\n",
    "    date = year + month\n",
    "    df = data_df[data_df['date']==date].reset_index(drop=True)\n",
    "    corpus = df['text'].tolist()\n",
    "    return ner_person(corpus)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import things that are needed generically\n",
    "from langchain import LLMMathChain, SerpAPIWrapper\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import BaseTool, StructuredTool, Tool, tool\n",
    "\n",
    "openai_api_key='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'\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "tools = [\n",
    "    StructuredTool.from_function(\n",
    "        func=retrieve_tool,\n",
    "        name = \"retrieval\",\n",
    "        description=\"useful for when you need to retrieve information on specific date. The input of month should be number of two digit. For example, February: 02.\"\n",
    "    ),\n",
    "    StructuredTool.from_function(\n",
    "        func=sentiment_analysis,\n",
    "        name = \"sentiment analysis\",\n",
    "        description=\"Useful for when you need to know the sentiment on specific date. The input of month should be number of two digit. For example, February: 02.\"\n",
    "    ),\n",
    "    StructuredTool.from_function(\n",
    "        func=ner_tool,\n",
    "        name = \"named entity recognition\",\n",
    "        description=\"Useful for when you need to know the most popular people on specific date. The input of month should be number of two digit. For example, February: 02.\"\n",
    "    ),\n",
    "]\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''\n",
    "I want to know the most popular people in February, 2023. \n",
    "Also, please tell me some information about the person at that time, and the sentiment toward him. \n",
    "Reply in traditional Chinese.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction:\n",
      "```\n",
      "{\n",
      "  \"action\": \"named entity recognition\",\n",
      "  \"action_input\": {\n",
      "    \"year\": \"2023\",\n",
      "    \"month\": \"02\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1020/1020 [00:00<00:00, 1685.73it/s]\n",
      "Inference: 100%|██████████| 23/23 [00:10<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[38;5;200m\u001b[1;3m林智堅\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction:\n",
      "```\n",
      "{\n",
      "  \"action\": \"retrieval\",\n",
      "  \"action_input\": {\n",
      "    \"year\": \"2023\",\n",
      "    \"month\": \"02\",\n",
      "    \"query\": \"林智堅\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1020/1020 [00:00<00:00, 1725.38it/s]\n",
      "Inference: 100%|██████████| 23/23 [00:10<00:00,  2.17it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 13530.01it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 150.28it/s]\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-ZUVi9bYEx0Di5RrvC2VXibwz on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3m1.新聞網址︰\n",
      "https://newtalk.tw/news/view/2023-02-13/857325\n",
      "2.新聞來源︰\n",
      "Newtalk\n",
      "3.完整新聞標題\n",
      "不大聲說了嗎？民進黨闢謠專區悄悄下架「林智堅沒有抄襲」頁面\n",
      "4.完整新聞內容︰\n",
      "前新竹市長林智堅論文抄襲案，目前雙雙進入司法程序，台大國發所論文1月中旬台北地\n",
      "院首度召開準備程序庭，傳喚被告林智堅到庭，中華大學論文也被國科會證實，竹科管理\n",
      "局已依法處理，且已進入司法程序。去年還是民進黨主席的總統蔡英文，曾要全體黨公職\n",
      "大聲地跟大家說「林智堅沒有抄襲」，並在民進黨網站闢謠專區中放上為林智堅辯護的頁\n",
      "面，不過，近日被發現已將林智堅論文相關貼文下架。\n",
      "針對林智堅論文抄襲爭議，總統蔡英文去年8月曾要求全體黨公職表態，「只要是完整看\n",
      "過兩本論文，而且完整了解事情來龍去脈的人，都願意選擇相信智堅沒有抄襲」，還呼籲\n",
      "黨公職一起「大聲地跟大家說」，讓社會可以理解整件事情的始末和真相。\n",
      "民進黨也在黨的網站「闢謠專區」頁面中，放上了「林智堅論文為原創」、「林智堅中華\n",
      "大學碩論享有著作權 且口試早於著作權轉移 沒有抄襲、欽權疑慮」等貼文，試圖為林智\n",
      "堅辯護。\n",
      "不過，隨著林智堅論文案先後被台灣大學與中華大學學倫會判定抄襲，案件也在調查局新\n",
      "竹市調查站調查官余正煌正式開告後進入司法程序，資深媒體人黃揚明9日曾在臉書發文\n",
      "表示，民進黨官網的「闢謠專區」仍留有堅稱前新竹市長林智堅論文沒抄襲的2篇文章。\n",
      "黃揚明呼籲，檢討學倫機制之外，也該把這些顛倒是非的內容下架。\n",
      "經過了四天，民進黨原先的「闢謠專區」頁面中，已悄悄將林智堅論文沒有抄襲，且為原\n",
      "創等貼文頁面下架。\n",
      "5.附註、心得、想法︰\n",
      "闢謠闢到全黨火葬場了吼\n",
      "是不是收到風聲台大的結果要出來了\n",
      "看過小智兩本論文的人現在還有呼吸嗎\n",
      "還好我下架前看過這原文了\n",
      "綠共吃屎\n",
      "1.新聞網址︰\n",
      "※ 網址超過一行 請縮網址 ※\n",
      "https://newtalk.tw/news/view/2023-02-15/857533\n",
      "2.新聞來源︰  新頭殼\n",
      "3.完整新聞標題\n",
      "※ 請完整轉載標題 請勿修改與編排 ※\n",
      "翁達瑞：若連林智堅都無法保護 平民百姓受冤屈豈更不能寄望民進黨\n",
      "4.完整新聞內容︰\n",
      "※ 請完整轉載原文 請勿修改內文與編排 ※\n",
      "前新竹市長林智堅論文抄襲風波被民進黨視為是九合一選舉大敗的主因之一，並在調查局\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction:\n",
      "```\n",
      "{\n",
      "  \"action\": \"retrieval\",\n",
      "  \"action_input\": {\n",
      "    \"year\": \"2023\",\n",
      "    \"month\": \"02\",\n",
      "    \"query\": \"林智堅\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1020/1020 [00:00<00:00, 2387.96it/s]\n",
      "Inference: 100%|██████████| 23/23 [00:10<00:00,  2.16it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 19972.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 143.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3m1.新聞網址︰\n",
      "https://newtalk.tw/news/view/2023-02-13/857325\n",
      "2.新聞來源︰\n",
      "Newtalk\n",
      "3.完整新聞標題\n",
      "不大聲說了嗎？民進黨闢謠專區悄悄下架「林智堅沒有抄襲」頁面\n",
      "4.完整新聞內容︰\n",
      "前新竹市長林智堅論文抄襲案，目前雙雙進入司法程序，台大國發所論文1月中旬台北地\n",
      "院首度召開準備程序庭，傳喚被告林智堅到庭，中華大學論文也被國科會證實，竹科管理\n",
      "局已依法處理，且已進入司法程序。去年還是民進黨主席的總統蔡英文，曾要全體黨公職\n",
      "大聲地跟大家說「林智堅沒有抄襲」，並在民進黨網站闢謠專區中放上為林智堅辯護的頁\n",
      "面，不過，近日被發現已將林智堅論文相關貼文下架。\n",
      "針對林智堅論文抄襲爭議，總統蔡英文去年8月曾要求全體黨公職表態，「只要是完整看\n",
      "過兩本論文，而且完整了解事情來龍去脈的人，都願意選擇相信智堅沒有抄襲」，還呼籲\n",
      "黨公職一起「大聲地跟大家說」，讓社會可以理解整件事情的始末和真相。\n",
      "民進黨也在黨的網站「闢謠專區」頁面中，放上了「林智堅論文為原創」、「林智堅中華\n",
      "大學碩論享有著作權 且口試早於著作權轉移 沒有抄襲、欽權疑慮」等貼文，試圖為林智\n",
      "堅辯護。\n",
      "不過，隨著林智堅論文案先後被台灣大學與中華大學學倫會判定抄襲，案件也在調查局新\n",
      "竹市調查站調查官余正煌正式開告後進入司法程序，資深媒體人黃揚明9日曾在臉書發文\n",
      "表示，民進黨官網的「闢謠專區」仍留有堅稱前新竹市長林智堅論文沒抄襲的2篇文章。\n",
      "黃揚明呼籲，檢討學倫機制之外，也該把這些顛倒是非的內容下架。\n",
      "經過了四天，民進黨原先的「闢謠專區」頁面中，已悄悄將林智堅論文沒有抄襲，且為原\n",
      "創等貼文頁面下架。\n",
      "5.附註、心得、想法︰\n",
      "闢謠闢到全黨火葬場了吼\n",
      "是不是收到風聲台大的結果要出來了\n",
      "看過小智兩本論文的人現在還有呼吸嗎\n",
      "還好我下架前看過這原文了\n",
      "綠共吃屎\n",
      "1.新聞網址︰\n",
      "※ 網址超過一行 請縮網址 ※\n",
      "https://newtalk.tw/news/view/2023-02-15/857533\n",
      "2.新聞來源︰  新頭殼\n",
      "3.完整新聞標題\n",
      "※ 請完整轉載標題 請勿修改與編排 ※\n",
      "翁達瑞：若連林智堅都無法保護 平民百姓受冤屈豈更不能寄望民進黨\n",
      "4.完整新聞內容︰\n",
      "※ 請完整轉載原文 請勿修改內文與編排 ※\n",
      "前新竹市長林智堅論文抄襲風波被民進黨視為是九合一選舉大敗的主因之一，並在調查局\n",
      "\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-ZUVi9bYEx0Di5RrvC2VXibwz on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-ZUVi9bYEx0Di5RrvC2VXibwz on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-ZUVi9bYEx0Di5RrvC2VXibwz on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mAction:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"2023年2月最受歡迎的人是林智堅。根據檢索到的新聞，林智堅在當時因論文抄襲案受到爭議。其中一篇新聞指出，民進黨闢謠專區悄悄下架了「林智堅沒有抄襲」頁面。另一篇新聞則報導了翁達瑞的言論，認為若連林智堅都無法保護，平民百姓受冤屈豈更不能寄望民進黨。\"\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2023年2月最受歡迎的人是林智堅。根據檢索到的新聞，林智堅在當時因論文抄襲案受到爭議。其中一篇新聞指出，民進黨闢謠專區悄悄下架了「林智堅沒有抄襲」頁面。另一篇新聞則報導了翁達瑞的言論，認為若連林智堅都無法保護，平民百姓受冤屈豈更不能寄望民進黨。'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "embedder = SentenceTransformer('GanymedeNil/text2vec-large-chinese', device=2 ,cache_folder='/nfs/nas-6.1/wclu/cache')\n",
    "\n",
    "corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True, normalize_embeddings=True, device=2, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_semantic(query):\n",
    "    query_embedding = embedder.encode([query], convert_to_tensor=True, normalize_embeddings=True, device=2, show_progress_bar=True)\n",
    "    hits = util.semantic_search(query_embedding, corpus_embeddings, score_function=util.dot_score, top_k=10)\n",
    "    rel_doc = [corpus[hit['corpus_id']] for hit in hits[0]]\n",
    "    return rel_doc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
