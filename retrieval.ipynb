{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from collections import Counter\n",
    "\n",
    "# Parse the XML data\n",
    "def get_text(doc_path):\n",
    "    tree = ET.parse(doc_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Extract metadata\n",
    "    metadata = root.find('./teiHeader')\n",
    "    metadata_dict = {}\n",
    "    for item in metadata.findall('metadata'):\n",
    "        name = item.get('name')\n",
    "        value = item.text\n",
    "        metadata_dict[name] = value\n",
    "\n",
    "    # Extract text content\n",
    "    text = root.find('./text')\n",
    "    body_author = text.find('./body').get('author')\n",
    "    title_author = text.find('./title').get('author')\n",
    "    sentences = text.findall('body/s')\n",
    "    comments = text.findall('comment')\n",
    "    comments_pairs = [([(word.get('type'), word.text) for word in c.findall('s/w')], c.get('c_type')) for c in comments]\n",
    "    sentences_pairs = [[(word.get('type'), word.text) for word in sent.findall('w')] for sent in sentences]\n",
    "    text = []\n",
    "    c = Counter()\n",
    "    for sentense in sentences_pairs:\n",
    "        sentense_parsed = ''.join([word[1] for word in sentense])\n",
    "        text.append(sentense_parsed)\n",
    "    text = '\\n'.join(text)\n",
    "    for comment in comments_pairs:\n",
    "        c[comment[1]] += 1\n",
    "    file_name = doc_path.split('/')[-1]\n",
    "    return {'date': file_name[:6], 'text': text, 'pos': c['pos'], 'neu': c['neu'], 'neg': c['neg']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "directory = '/nfs/nas-6.1/wclu/cllt/ptt_data/HatePolitics'\n",
    "data = []\n",
    " \n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for filename in tqdm(files):\n",
    "        if filename != '.DS_Store':\n",
    "            doc_path = os.path.join(root, filename)\n",
    "            try:\n",
    "                data.append(get_text(doc_path))\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "import pandas as pd\n",
    "data_df = pd.DataFrame(data)\n",
    "data_df['num_com'] = data_df['pos'] + data_df['neu'] + data_df['neg']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ckip_transformers.nlp import CkipWordSegmenter\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "ws_driver  = CkipWordSegmenter(model=\"bert-base\", device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def retrieve_bm25(year, month, query):\n",
    "    date = year + month\n",
    "    df = data_df[data_df['date']==date].reset_index(drop=True)\n",
    "    corpus = df['text'].tolist()\n",
    "    corpus_tokenized = ws_driver(corpus, batch_size=64, max_length=509)\n",
    "    bm25 = BM25Okapi(corpus_tokenized)\n",
    "    tokenized_query = ws_driver([query], batch_size=1, max_length=509)[0]\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    top_n = np.argsort(scores)[::-1][:20]\n",
    "    rel_doc = df.iloc[top_n].sort_values(by=['num_com'], ascending=False).head(5)\n",
    "    return rel_doc.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_doc = retrieve_bm25('2023', '02', '林智堅')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer=BertTokenizer.from_pretrained('IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment', cache_dir='/nfs/nas-6.1/wclu/cache')\n",
    "model=BertForSequenceClassification.from_pretrained('IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment', cache_dir='/nfs/nas-6.1/wclu/cache')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: 'negative',\n",
    "    1: 'positive'\n",
    "}\n",
    "def sentiment_analysis(year, month, query):\n",
    "    rel_doc = retrieve_bm25(year, month, query)\n",
    "    doc_text = rel_doc['text'].tolist()\n",
    "    x = tokenizer(doc_text, padding='longest', truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    output = model(x['input_ids'])\n",
    "    sentiment = []\n",
    "    for logit in output.logits:\n",
    "        sentiment.append(id2label[int(logit.argmax())])\n",
    "    rel_doc['senti'] = sentiment\n",
    "    return sentiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def retrieve_tool(year, month, query):\n",
    "    rel_doc = retrieve_bm25(year, month, query)\n",
    "    return rel_doc['text'].tolist()[0][:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import things that are needed generically\n",
    "from langchain import LLMMathChain, SerpAPIWrapper\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import BaseTool, StructuredTool, Tool, tool\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key='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')\n",
    "tools = [\n",
    "    StructuredTool.from_function(\n",
    "        func=retrieve_tool,\n",
    "        name = \"retrieve\",\n",
    "        description=\"useful for when you need to retrieve documents on specific date\"\n",
    "    ),\n",
    "    StructuredTool.from_function(\n",
    "        func=sentiment_analysis,\n",
    "        name = \"sentiment analysis\",\n",
    "        description=\"useful for when you need to know the sentiment on specific date\"\n",
    "    ),\n",
    "]\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction:\n",
      "```\n",
      "{\n",
      "  \"action\": \"retrieve\",\n",
      "  \"action_input\": {\n",
      "    \"year\": \"2023\",\n",
      "    \"month\": \"02\",\n",
      "    \"query\": \"林智堅\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1020/1020 [00:00<00:00, 1471.33it/s]\n",
      "Inference: 100%|██████████| 23/23 [00:10<00:00,  2.21it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 22429.43it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 144.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3m1.新聞網址︰\n",
      "https://newtalk.tw/news/view/2023-02-13/857325\n",
      "2.新聞來源︰\n",
      "Newtalk\n",
      "3.完整新聞標題\n",
      "不大聲說了嗎？民進黨闢謠專區悄悄下架「林智堅沒有抄襲」頁面\n",
      "4.完整新聞內容︰\n",
      "前新竹市長林智堅論文抄襲案，目前雙雙進入司法程序，台大國發所論文1月中旬台北地\n",
      "院首度召開準備程序庭，傳喚被告林智堅到庭，中華大學論文也被國科會證實，竹科管理\n",
      "局已依法處理，且已進入司法程序。去年還是民進黨主席的總統蔡英文，曾要全體黨公職\n",
      "大聲地跟大家說「林智堅沒有抄襲」，並在民進黨網站闢謠專區中放上為林智堅辯護的頁\n",
      "面，不過，近日被發現已將林智堅論文相關貼文下架。\n",
      "針對林智堅論文抄襲爭議，總統蔡英文去年8月曾要求全體黨公職表態，「只要是完整看\n",
      "過兩本論文，而且完整了解事情來龍去脈的人，都願意選擇相信智堅沒有抄襲」，還呼籲\n",
      "黨公職一起「大聲地跟大家說」，讓社會可以理解整件事情的始末和真相。\n",
      "民進黨也在黨的網站「闢謠專區」頁面中，放上了「林智堅論文為原創」、「林智堅中華\n",
      "大學碩論享有著作權 且口試早於著作權轉移 沒有抄襲、欽權疑慮」等貼文，試圖為林智\n",
      "堅辯護。\n",
      "不過，隨著林智堅論文案先後被台灣大學與中華大學學倫會判定抄襲，案件也在調查局新\n",
      "竹市調查站調查官余正煌正式開告後進入司法程序，資深媒體人黃揚明9日曾在臉書發文\n",
      "表示，民進黨官網的「闢謠專區」仍留有堅稱前新竹市長林智堅論文沒抄襲的2篇文章。\n",
      "黃揚明呼籲，檢討學倫機制之外，也該把這些顛倒是非的內容下架。\n",
      "經過了四天，民進黨原先的「闢謠專區」頁面中，已悄悄將林智堅論文沒有抄襲，且為原\n",
      "創等貼文頁面下架。\n",
      "5.附註、心得、想法︰\n",
      "闢謠闢到全黨火葬場了吼\n",
      "是不是收到風聲台大的結果要出來了\n",
      "看過小智兩本論文的人現在還有呼吸嗎\n",
      "還好我下架前看過這原文了\n",
      "綠共吃屎\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe user requested to retrieve documents about 林智堅 on 02, 2023 and we provided a news article about the controversy surrounding his alleged plagiarism. \n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"The retrieved document is a news article from Newtalk about the controversy surrounding 林智堅's alleged plagiarism in his thesis. The article discusses how the DPP party had a page on their website defending him, but it has since been taken down. \"\n",
      "}\n",
      "```\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The retrieved document is a news article from Newtalk about the controversy surrounding 林智堅's alleged plagiarism in his thesis. The article discusses how the DPP party had a page on their website defending him, but it has since been taken down. \""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"I want to retrieve documents about 林智堅 on 02, 2023\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "embedder = SentenceTransformer('GanymedeNil/text2vec-large-chinese', device=2 ,cache_folder='/nfs/nas-6.1/wclu/cache')\n",
    "\n",
    "corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True, normalize_embeddings=True, device=2, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_semantic(query):\n",
    "    query_embedding = embedder.encode([query], convert_to_tensor=True, normalize_embeddings=True, device=2, show_progress_bar=True)\n",
    "    hits = util.semantic_search(query_embedding, corpus_embeddings, score_function=util.dot_score, top_k=10)\n",
    "    rel_doc = [corpus[hit['corpus_id']] for hit in hits[0]]\n",
    "    return rel_doc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
