{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from collections import Counter\n",
    "\n",
    "# Parse the XML data\n",
    "def get_text(doc_path):\n",
    "    tree = ET.parse(doc_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Extract metadata\n",
    "    metadata = root.find('./teiHeader')\n",
    "    metadata_dict = {}\n",
    "    for item in metadata.findall('metadata'):\n",
    "        name = item.get('name')\n",
    "        value = item.text\n",
    "        metadata_dict[name] = value\n",
    "\n",
    "    # Extract text content\n",
    "    text = root.find('./text')\n",
    "    body_author = text.find('./body').get('author')\n",
    "    title_author = text.find('./title').get('author')\n",
    "    sentences = text.findall('body/s')\n",
    "    comments = text.findall('comment')\n",
    "    comments_pairs = [([(word.get('type'), word.text) for word in c.findall('s/w')], c.get('c_type')) for c in comments]\n",
    "    sentences_pairs = [[(word.get('type'), word.text) for word in sent.findall('w')] for sent in sentences]\n",
    "    text = []\n",
    "    c = Counter()\n",
    "    for sentense in sentences_pairs:\n",
    "        sentense_parsed = ''.join([word[1] for word in sentense])\n",
    "        text.append(sentense_parsed)\n",
    "    text = '\\n'.join(text)\n",
    "    for comment in comments_pairs:\n",
    "        c[comment[1]] += 1\n",
    "    file_name = doc_path.split('/')[-1]\n",
    "    return {'date': file_name[:6], 'text': text, 'pos': c['pos'], 'neu': c['neu'], 'neg': c['neg']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4102ab1012f64e75a86aeaa66eede189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f1192a1c4944a49c599a7c981da01e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3815 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d3da49ad13f4c87a679b3c063ccc915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3780 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb5468a09ffc4069b8d2baadb4d577d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "446711cdf9e3419f82945a405503c80e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1178 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "directory = '/nfs/nas-6.1/wclu/cllt/ptt_data/HatePolitics'\n",
    "data = []\n",
    " \n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for filename in tqdm(files):\n",
    "        if filename != '.DS_Store':\n",
    "            doc_path = os.path.join(root, filename)\n",
    "            try:\n",
    "                data.append(get_text(doc_path))\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "import pandas as pd\n",
    "data_df = pd.DataFrame(data)\n",
    "data_df['num_com'] = data_df['pos'] + data_df['neu'] + data_df['neg']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ckip_transformers.nlp import CkipWordSegmenter\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "ws_driver  = CkipWordSegmenter(model=\"bert-base\", device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def retrieve_bm25(year, month, query):\n",
    "    date = year + month\n",
    "    df = data_df[data_df['date']==date].reset_index(drop=True)\n",
    "    corpus = df['text'].tolist()\n",
    "    corpus_tokenized = ws_driver(corpus, batch_size=64, max_length=509)\n",
    "    bm25 = BM25Okapi(corpus_tokenized)\n",
    "    tokenized_query = ws_driver([query], batch_size=1, max_length=509)[0]\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    top_n = np.argsort(scores)[::-1][:20]\n",
    "    rel_doc = df.iloc[top_n].sort_values(by=['num_com'], ascending=False).head(5)\n",
    "    return rel_doc.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1020/1020 [00:00<00:00, 1534.72it/s]\n",
      "Inference: 100%|██████████| 23/23 [00:10<00:00,  2.21it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 22671.91it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 156.54it/s]\n"
     ]
    }
   ],
   "source": [
    "rel_doc = retrieve_bm25('2023', '02', '林智堅')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.轉錄網址︰\n",
      "※ 網址超過一行 請縮網址 ※\n",
      "https://bit.ly/3RZt0HO\n",
      "2.轉錄來源︰\n",
      "※ FB公眾人物、FB粉絲團名稱、其他來源 ※\n",
      "ㄌ翁達瑞臉書\n",
      "3.轉錄內容︰\n",
      "※ 請完整轉載原文 請勿修改內文與編排 ※\n",
      "尊重  但不捨林智堅的決定\n",
      "翁達瑞 / 美國大學教授\n",
      "教育部駁回中華的學倫調查訴願，林智堅決定不提行政訴訟。林智堅也決定撤回在教育部的\n",
      "台大學倫調查訴願。這表示林智堅選擇吞下他的冤屈。\n",
      "對林智堅的決定，我尊重，但仍感到萬分不捨。\n",
      "#林智堅的冤屈\n",
      "林智堅在中華與台大的碩士論文，撰寫的過程「或有瑕疵」，但問題絕對不在抄襲。兩校以\n",
      "抄襲的罪名撤銷林智堅的學位，皆是不可承受的冤屈。\n",
      "中華與台大做出抄襲判定後，林智堅面臨龐大的壓力。黨內有人要他認錯道歉，避免波及選\n",
      "情。林智堅最後決定退選，以平民身分找回清白。\n",
      "儘管選舉已結束，林智堅面對的壓力沒有減輕。敗選之後，民進黨內「清理戰場」的聲音擴\n",
      "大。林智堅反變成敗選的戰犯。\n",
      "民進黨官網的「闢謠專區」本來有一篇公告，為林智堅的清白澄清。近日，這篇公告已被撤\n",
      "下。這應該是壓垮林智堅的最後一根稻草。\n",
      "我跟林智堅非親非故，原本就不相識。我跳出來力挺他，純屬路見不平。中華與台大的學倫\n",
      "調查泛政治化，將抄襲罪名強加在林智堅身上。\n",
      "兩校的調查都有嚴重的瑕疵。中華的調查叫「偷天換日」；台大的調查則是「私吞證據」。\n",
      "為了留下歷史紀錄，我把林智堅遭受的冤屈詳述如下：\n",
      "#中華偷天換日\n",
      "從一開始，我就說中華的調查「名不符實」，教育部的複審「虛實不分」。這個案子整個被\n",
      "「偷天換日」了。\n",
      "林智堅的碩士論文與竹科的專案報告，是台灣學界常見的「一稿兩用」。既然是一稿兩用，\n",
      "內容當然大部分雷同，抄襲根本不是爭議之所在。\n",
      "中華可以質疑林智堅的「一稿兩用」是否過當，但調查重點應放在教授的專案轉包過程，而\n",
      "非林智堅是否涉及文字抄襲。\n",
      "中華大學偷天換日的調查，就是縱放教授，冤屈學生。可歎的是，教育部的訴願審查也虛實\n",
      "不分，全盤接受一個名不符實的調查結論。\n",
      "中華的調查可用一個「虛構」的例子類比：\n",
      "多年前，我向中華大學的教授轉租教員宿舍。該教授聲稱轉租未違反規定，中華大學也知情\n",
      "。談妥轉租條件後我就入住，但並未簽署租約。\n",
      "租用結束多年後，我投身政治，成為閃亮的政壇明星。為了打擊我，選舉對手向中華大學檢\n",
      "舉我曾佔用該校的教員宿舍。\n",
      "中華大學啟動「侵佔」調查。當初轉租宿舍給我的教授，也提出我確實入住該宿舍的證據。\n",
      "因為我入住的證據明確，中華大學判定「侵佔」成立。\n",
      "這是一起「轉租」糾紛，卻被用「侵佔」的名義調查。林智堅在中華的論文爭議是「一稿兩\n",
      "用」，卻被用「文字抄襲」的名義調查。這就是我說的「偷天換日」。\n",
      "#台大私吞證據\n",
      "針對林智堅與余正煌的論文抄襲爭議，我一開始就說抄襲確實存在，疑問在林抄余、余抄林\n",
      "、或林余都抄第三人。\n",
      "台大只根據兩人的論文口試時間，判定林智堅抄襲余正煌。問題是，論文雷同的內容皆屬研\n",
      "究方法的描述，轉寫時序遠早於論文口試時間。\n",
      "台大的調查刻意忽略兩份文稿：一份是陳明通提供給余正煌的所謂「論文公版」；另一份是\n",
      "林智堅的研究計畫。兩份文稿的時序都早於余正煌的論文口試。\n",
      "台大判定林智堅抄襲余正煌的文字，在這兩份更早的文稿都可以發現。如果台大採信這兩份\n",
      "文稿，那就是余正煌抄襲陳明通與林智堅。\n",
      "台大拒絕採信林智堅的研究計畫，理由是林智堅無法證明他是原創者。即便林智堅的研究計\n",
      "畫也出自陳明通，那也是余正煌抄襲陳明通，不會是林智堅抄襲余正煌。\n",
      "根據我的判斷，林智堅在教育部的訴願應有很大的勝算，因為白紙黑字的證據無從否定。林\n",
      "智堅決定撤回訴願應是政治考量，因為不管結果如何，民進黨都是輸家。\n",
      "如果林智堅訴願輸了，這會進一步坐實他的抄襲罪名，民進黨要概括承受。如果林智堅訴願\n",
      "贏了，民進黨會被指控政治介入台大的學倫調查。在民進黨雙輸的壓力下，林智堅決定吞下\n",
      "冤屈。\n",
      "#給民進黨的諍言\n",
      "在多個公開場合，我大方承認我是「百分之百」的民進黨支持者，因為民進黨的理念與政策\n",
      "，符合我對台灣未來的期待。\n",
      "事實上，我支持的不是民進黨，更不是民進黨籍的政治人物。我支持的是民進黨的理念，包\n",
      "括對「是非公理」的堅持。\n",
      "針對林智堅蒙受的抄襲冤屈，我認為民進黨的選舉考量多於對是非公理的堅持。這樣的表現\n",
      "不像我支持的民進黨。\n",
      "林智堅是否涉及抄襲，民進黨可輕易判定，因為白紙黑字的證據充足。依據多年的學界經歷\n",
      "，我認定林智堅並未涉及抄襲，即便他的論文有其他瑕疵。\n",
      "假如民進黨也有相同的認定，卻不願幫助林智堅洗刷冤屈，這讓我感到失望。如果民進黨仍\n",
      "不了解林智堅的冤屈，這也讓我失望。\n",
      "林智堅是民進黨的重要從政黨員。若民進黨連林智堅都無法保護，那平民百姓受到冤屈時，\n",
      "豈更不能寄望民進黨的保護。\n",
      "民進黨真要清理論文抄襲的戰場，應該站在「是非公理」的格局，而不是端出「切結審核」\n",
      "的雕蟲小技。\n",
      "政治人物被控論文抄襲，各黨派都有，背後有複雜的文化、法律、政治、司法、教育、媒體\n",
      "等結構性的問題。這是台灣的歷史共業，不是民進黨專有的政治包袱。\n",
      "民進黨要清理論文抄襲的戰場，應聚焦在「政治人物崇拜學歷」、「高等教育商業化」、與\n",
      "「高教法令落後過時」等面向。這三者都是大題目，不是一黨一派所能解決。\n",
      "選舉勝負固然重要，但台灣社會也不能沒有是非公理。這是我給民進黨的諍言。\n",
      "#給林智堅的擁抱\n",
      "林智堅投入政治多年，但仍保有台灣人傳統的厚道與靦腆。正因為這樣，林智堅遭受的冤屈\n",
      "更讓我感到不捨。\n",
      "撤回了教育部的訴願，林智堅還要繼續面對余正煌的著作權訴訟。我判斷余正煌會撤回告訴\n",
      "，因為沒有勝訴的證據。如果余正煌沒撤告，我預期林智堅會贏回該有的清白。\n",
      "對林智堅不提行政訴訟與撤回訴願的決定，我雖然感到不捨，但充分尊重。末了，讓我給林\n",
      "智堅一個深深的擁抱！\n",
      "4.附註、心得、想法︰\n",
      "※ 40字心得、備註 ※\n",
      "唉\n",
      "翁教授說話了\n",
      "看起來很心疼小智\n",
      "對於中華偷天換日 台大私吞證據讓小智必須吞下這分冤屈\n",
      "#擁抱林智堅\n",
      "#小智沒有輸\n",
      "※ 「Live」、「新聞」、「轉錄」此類文章每日發文數總上限為3篇，\n",
      "自刪與板主刪除，同樣計入額度 ※\n",
      "Sent from my iPhone 12 Pro Max\n",
      "○ PiTT // PHJCI\n"
     ]
    }
   ],
   "source": [
    "print(rel_doc['text'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer=BertTokenizer.from_pretrained('IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment', cache_dir='/nfs/nas-6.1/wclu/cache')\n",
    "model=BertForSequenceClassification.from_pretrained('IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment', cache_dir='/nfs/nas-6.1/wclu/cache')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: 'negative',\n",
    "    1: 'positive'\n",
    "}\n",
    "def sentiment_analysis(year, month, query):\n",
    "    rel_doc = retrieve_bm25(query)\n",
    "    doc_text = rel_doc['text'].tolist()\n",
    "    x = tokenizer(doc_text, padding='longest', truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    output = model(x['input_ids'])\n",
    "    sentiment = []\n",
    "    for logit in output.logits:\n",
    "        sentiment.append(id2label[int(logit.argmax())])\n",
    "    rel_doc['senti'] = sentiment\n",
    "    return sentiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def retrieve_tool(query):\n",
    "    tokenized_query = ws_driver([query], batch_size=1, max_length=509)[0]\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    top_n = np.argsort(scores)[::-1][:50]\n",
    "    rel_doc = data_df.iloc[top_n].sort_values(by=['num_com'], ascending=False).head(1)\n",
    "    return rel_doc['text'].tolist()[0][:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import things that are needed generically\n",
    "from langchain import LLMMathChain, SerpAPIWrapper\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import BaseTool, StructuredTool, Tool, tool\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key='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')\n",
    "tools = [\n",
    "    Tool.from_function(\n",
    "        func=retrieve_tool,\n",
    "        name = \"Retrieve\",\n",
    "        description=\"useful for when you need to retrieve documents\"\n",
    "    ),\n",
    "    StructuredTool.from_function(\n",
    "        func=sentiment_analysis,\n",
    "        name = \"sentiment analysis\",\n",
    "        description=\"useful for when you need to know the sentiment\"\n",
    "    ),\n",
    "]\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(\"I want to know the sentiment about 林智堅\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "embedder = SentenceTransformer('GanymedeNil/text2vec-large-chinese', device=2 ,cache_folder='/nfs/nas-6.1/wclu/cache')\n",
    "\n",
    "corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True, normalize_embeddings=True, device=2, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_semantic(query):\n",
    "    query_embedding = embedder.encode([query], convert_to_tensor=True, normalize_embeddings=True, device=2, show_progress_bar=True)\n",
    "    hits = util.semantic_search(query_embedding, corpus_embeddings, score_function=util.dot_score, top_k=10)\n",
    "    rel_doc = [corpus[hit['corpus_id']] for hit in hits[0]]\n",
    "    return rel_doc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
